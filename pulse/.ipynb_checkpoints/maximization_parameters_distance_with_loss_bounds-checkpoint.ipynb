{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (fc1): Linear(in_features=6, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (fc4): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (fc5): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Tuple\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from model import Net\n",
    "model = torch.load('model_weights/modelentire.pth')\n",
    "model.load_state_dict(torch.load('model_weights/model.pth'))\n",
    "loss_function = nn.MSELoss()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_mse(u_exp: np.ndarray, u: np.ndarray, time: np.ndarray, eps: float = + 10 ** (-15)) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    dt = np.diff(time, n=1, axis=0)\n",
    "    dt = np.append(dt, dt[-1])\n",
    "    amplitude = np.dot(dt, u * u_exp) / np.dot(dt, u ** 2)\n",
    "    e = (u_exp - amplitude * u) / (u_exp + eps) * dt\n",
    "    \n",
    "    loss = np.dot(e, e)\n",
    "#     if (np.log(loss) == math.inf):\n",
    "#         print('u_exp\\n', u_exp)\n",
    "#         print('u\\n', u)\n",
    "#         print('amplitude = ', amplitude)\n",
    "#         print ('loss = ', loss)\n",
    "#         sys.exit()\n",
    "    return loss, amplitude, e\n",
    "\n",
    "\n",
    "\n",
    "t = 1e-3 * np.array([\n",
    "    0.135, 0.180, 0.235, 0.315, 0.365, 0.420,\n",
    "    0.485, 0.560, 0.650, 0.750, 0.865, 1.000,\n",
    "    1.155, 1.335, 1.540, 1.780, 2.055, 2.370,\n",
    "    2.740, 3.160, 3.650, 4.215, 4.870, 5.625,\n",
    "    6.495, 7.500, 8.660, 10.00, 11.55, 13.34,\n",
    "    15.40, 17.79, 20.54, 23.72, 27.39, 31.63,\n",
    "    36.52, 42.17, 48.70, 56.24, 64.94, 74.99,\n",
    "    86.60, 100.0, 115.5, 133.4\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexander/anaconda3/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729096996/work/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')\n",
    "    else:\n",
    "        device = torch.device('cpu') # don't have GPU \n",
    "    return device\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "def appending_to_tensor(tensor, fix_vector, variable_vector):\n",
    "    tmp_tensor = torch.tensor([rand_2_strs.get('Sigma').iloc[0],rand_2_strs.get('Sigma').iloc[1],\n",
    "                               rand_2_strs.get('mu').iloc[0],rand_2_strs.get('mu').iloc[1],\n",
    "                               rand_2_strs.get('th').iloc[0],rand_2_strs.get('th').iloc[1],\n",
    "                               np.log(weighted_mse(np.array(rand_2_strs.get('Decay').iloc[0]),np.array(rand_2_strs.get('Decay').iloc[1]),t)[0])])\n",
    "    tmp_tensor = torch.unsqueeze(tmp_tensor, 0)\n",
    "    tmp_tensor = torch.cat((tensor,tmp_tensor),0)\n",
    "    return(tmp_tensor)\n",
    "\n",
    "def mean_absolute_percentage_error(y_pred, y_true, eps=10**(-2)): \n",
    "    return abs(torch.mean(((y_true-y_pred))/(y_true + eps)))*100\n",
    "\n",
    "\n",
    "def create_batches_to_device(df, device, num_fix_string,batch_size=1):\n",
    "    tensor = torch.tensor([])\n",
    "    fix_vector = \n",
    "    variable_vector = \n",
    "    tensor = appending_to_tensor(tensor, rand_train_str)\n",
    "    tensor = tensor.to(device)\n",
    "    trainset = torch.utils.data.DataLoader(tensor, batch_size=batch_size, shuffle=True)\n",
    "    return (trainset)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sigma</th>\n",
       "      <th>mu</th>\n",
       "      <th>d</th>\n",
       "      <th>th</th>\n",
       "      <th>th0</th>\n",
       "      <th>Decay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19628</th>\n",
       "      <td>-1.271194</td>\n",
       "      <td>-0.747548</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.085085</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.19375629692752577, 0.18294351872284478, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2486</th>\n",
       "      <td>-0.522897</td>\n",
       "      <td>0.229992</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.732977</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.192993357268207, 0.1820247479384618, 0.1711...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64883</th>\n",
       "      <td>1.436271</td>\n",
       "      <td>-0.737692</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.091825</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.1752133606610032, 0.16091191549296152, 0.14...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10027</th>\n",
       "      <td>1.629299</td>\n",
       "      <td>1.329376</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.526841</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.18759459262951242, 0.17560025481240513, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51126</th>\n",
       "      <td>0.423736</td>\n",
       "      <td>-0.777624</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.247653</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.17945400597800243, 0.1659367937378173, 0.15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69892</th>\n",
       "      <td>0.814175</td>\n",
       "      <td>0.781662</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.878862</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.1882601086365116, 0.17643418646482018, 0.16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72626</th>\n",
       "      <td>-0.020505</td>\n",
       "      <td>-0.181898</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.651101</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.18716586487573802, 0.17512817120621807, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17249</th>\n",
       "      <td>0.133407</td>\n",
       "      <td>1.380844</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.863656</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.1928662308439771, 0.18225947646392704, 0.17...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38159</th>\n",
       "      <td>1.210575</td>\n",
       "      <td>-0.099357</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.416446</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.1803007929163741, 0.16759557921691812, 0.15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67661</th>\n",
       "      <td>1.430852</td>\n",
       "      <td>-0.909071</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.060979</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.17361789585296816, 0.15884240040603975, 0.1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75200 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Sigma        mu    d        th  th0  \\\n",
       "19628 -1.271194 -0.747548  0.0 -1.085085  0.0   \n",
       "2486  -0.522897  0.229992  0.0 -0.732977  0.0   \n",
       "64883  1.436271 -0.737692  0.0  0.091825  0.0   \n",
       "10027  1.629299  1.329376  0.0 -1.526841  0.0   \n",
       "51126  0.423736 -0.777624  0.0 -0.247653  0.0   \n",
       "...         ...       ...  ...       ...  ...   \n",
       "69892  0.814175  0.781662  0.0 -0.878862  0.0   \n",
       "72626 -0.020505 -0.181898  0.0 -0.651101  0.0   \n",
       "17249  0.133407  1.380844  0.0  0.863656  0.0   \n",
       "38159  1.210575 -0.099357  0.0  1.416446  0.0   \n",
       "67661  1.430852 -0.909071  0.0 -1.060979  0.0   \n",
       "\n",
       "                                                   Decay  \n",
       "19628  [0.19375629692752577, 0.18294351872284478, 0.1...  \n",
       "2486   [0.192993357268207, 0.1820247479384618, 0.1711...  \n",
       "64883  [0.1752133606610032, 0.16091191549296152, 0.14...  \n",
       "10027  [0.18759459262951242, 0.17560025481240513, 0.1...  \n",
       "51126  [0.17945400597800243, 0.1659367937378173, 0.15...  \n",
       "...                                                  ...  \n",
       "69892  [0.1882601086365116, 0.17643418646482018, 0.16...  \n",
       "72626  [0.18716586487573802, 0.17512817120621807, 0.1...  \n",
       "17249  [0.1928662308439771, 0.18225947646392704, 0.17...  \n",
       "38159  [0.1803007929163741, 0.16759557921691812, 0.15...  \n",
       "67661  [0.17361789585296816, 0.15884240040603975, 0.1...  \n",
       "\n",
       "[75200 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle('Dataset')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "fix_str = df.sample(1)\n",
    "#fix_vector = np.array([fix_str.get('Sigma').iloc[0], fix_str.get('mu').iloc[0], fix_str.get('th').iloc[0]])\n",
    "data_for_plotting = []\n",
    "for _,row in df.iterrows():\n",
    "    norm_2d = np.linalg.norm([fix_str.get('Sigma').iloc[0] - row['Sigma'],\n",
    "                            fix_str.get('mu').iloc[0] - row['mu'],\n",
    "                              fix_str.get('th').iloc[0] - row['th']])\n",
    "    the_point = [row['Sigma'],row['mu'],row['th'],norm_2d]\n",
    "    data_for_plotting.append(the_point) \n",
    "print(data_for_plotting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.513514946355972"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fix_vector = np.array(fix_vector)\n",
    "new_vector = np.array([row.get('Sigma'),row.get('mu'),row.get('th')])\n",
    "fix_str.get('Sigma').iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def E()\n",
    "net = model.double().to(device)\n",
    "data_batch = create_batches_to_device(df, device, num_fix_string)\n",
    "for data in data_batch:\n",
    "    X = data[:, 0:-1]\n",
    "    y = data[:,-1]\n",
    "    output = net(X.double())\n",
    "    loss = loss_function(output.view(-1), y)\n",
    "    mape = mean_absolute_percentage_error(output.view(-1), y)\n",
    "    df_mape_loss.append(mape.item())\n",
    "    batch = batch + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
